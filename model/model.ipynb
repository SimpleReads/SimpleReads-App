{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 2 Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code reference: https://www.philschmid.de/sagemaker-llama2-qlora*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Riley\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Get the token from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get the token from the environment variable called HF_TOKEN\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "!huggingface-cli login --token {hf_token}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::960115796077:role/service-role/AmazonSageMaker-ExecutionRole-20230817T193823\n",
      "sagemaker bucket: sagemaker-us-east-1-960115796077\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "aws_defaulted_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "boto_session = boto3.Session(region_name=aws_defaulted_region)\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = os.environ['SAGEMAKER_ROLE_ARN']\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (C:/Users/Riley/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'Classify if the following are hollywood or bollywood movies: Zanjeer; The Shawshank redemption; Sholay; Pride & prejudice', 'context': '', 'response': 'Zanjeer and Sholay are bollywood moves. The Shawshank redemption and Pride & Prejudice are hollywood movies.', 'category': 'classification'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data For Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "classify the following spending items as either grocery, entertainment, food & drink, travel, bills: costco, netflix, uber, restaurant, rent, lyft\n",
      "\n",
      "### Answer\n",
      "costco should be grocery, netflix should be entertainment, uber should be travel, restaurant should be food & drink, rent should be bills, lyft should be travel\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Riley\\.cache\\huggingface\\datasets\\databricks___json\\databricks--databricks-dolly-15k-7427aa6e57c34282\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-363d4a18be8ac9f9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Riley\\.cache\\huggingface\\datasets\\databricks___json\\databricks--databricks-dolly-15k-7427aa6e57c34282\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-1b8af4c158afb96c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Riley\\.cache\\huggingface\\datasets\\databricks___json\\databricks--databricks-dolly-15k-7427aa6e57c34282\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-8081b3db3a9faf0c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is grep used for?\n",
      "\n",
      "### Context\n",
      "grep is a command-line utility for searching plain-text data sets for lines that match a regular expression. Its name comes from the ed command g/re/p (globally search for a regular expression and print matching lines), which has the same effect. \"grep\" was originally developed for the Unix operating system, but later available for all Unix-like systems and some others such as OS-9.\n",
      "\n",
      "### Answer\n",
      "grep is a popular Unix command that is used to search plain-text data sets for lines that match a pattern or regular expression. grep (global regular expression print) was originally developed for the Unix operating system, but later available for all Unix-like systems.\n",
      "\n",
      "An example of using the grep command to search for a keyword in a file: grep keyword file.txt\n",
      "\n",
      "You can pipe the output of other commands into grep. For example, grep can be used to search a directory for a file: ls ~/Desktop | grep keyword</s>\n",
      "Total number of samples: 1581\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-960115796077/processed/llama/dolly/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/dolly/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-08-21-01-13-50-2023-08-20-15-13-50-745\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.4xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m: training_input_path}\n\u001b[0;32m      4\u001b[0m \u001b[39m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m huggingface_estimator\u001b[39m.\u001b[39;49mfit(data, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:1293\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_for_training(job_name\u001b[39m=\u001b[39mjob_name)\n\u001b[0;32m   1292\u001b[0m experiment_config \u001b[39m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m-> 1293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job \u001b[39m=\u001b[39m _TrainingJob\u001b[39m.\u001b[39;49mstart_new(\u001b[39mself\u001b[39;49m, inputs, experiment_config)\n\u001b[0;32m   1294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[0;32m   1295\u001b[0m \u001b[39mif\u001b[39;00m wait:\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:2259\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[1;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[0;32m   2234\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m \n\u001b[0;32m   2236\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2255\u001b[0m \u001b[39m    all information about the started training job.\u001b[39;00m\n\u001b[0;32m   2256\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2257\u001b[0m train_args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[1;32m-> 2259\u001b[0m estimator\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_args)\n\u001b[0;32m   2261\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(estimator\u001b[39m.\u001b[39msagemaker_session, estimator\u001b[39m.\u001b[39m_current_job_name)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:892\u001b[0m, in \u001b[0;36mSession.train\u001b[1;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[0;32m    889\u001b[0m     LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mtrain request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[0;32m    890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_training_job(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest)\n\u001b[1;32m--> 892\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_create_request(train_request, submit, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:5494\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[1;34m(self, request, create, func_name)\u001b[0m\n\u001b[0;32m   5477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_intercept_create_request\u001b[39m(\n\u001b[0;32m   5478\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5479\u001b[0m     request: typing\u001b[39m.\u001b[39mDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5482\u001b[0m     \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[0;32m   5483\u001b[0m ):\n\u001b[0;32m   5484\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[0;32m   5485\u001b[0m \n\u001b[0;32m   5486\u001b[0m \u001b[39m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5492\u001b[0m \u001b[39m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[0;32m   5493\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5494\u001b[0m     \u001b[39mreturn\u001b[39;00m create(request)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:890\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m    888\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating training-job with name: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, job_name)\n\u001b[0;32m    889\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mtrain request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m--> 890\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_training_job(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Riley\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g5.4xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please use AWS Service Quotas to request an increase for this quota. If AWS Service Quotas is not available, contact AWS support to request an increase for this quota."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
